{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8xidxi0sp2kKo6yemg3tJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/AAAI_2024/blob/main/Specific_Sections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Looking at Fixations that Lie in Specific Sections"
      ],
      "metadata": {
        "id": "iLNDQnQio0JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Embedding, Concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, TimeDistributed, Embedding, Concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import scipy.stats as stats\n",
        "import statistics"
      ],
      "metadata": {
        "id": "x_d3gnOko5I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Filtering into Specific Sections"
      ],
      "metadata": {
        "id": "0Qd-PVXao8y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtereddata(urleye, urlhand):\n",
        "  eye_data=pd.read_excel(urleye)\n",
        "  hand_data=pd.read_excel(urlhand)\n",
        "  eye=eye_data.copy()\n",
        "  hand=hand_data.copy()\n",
        "  np.array(eye)\n",
        "  np.array(hand)\n",
        "  #each area is described as a rectangle with [Xmin, Ymin, Xmax, Ymax]\n",
        "  backgroundareae=[0, 0.09, 1, 0.462]\n",
        "  backgroundareah=[0, 0.565, 1, 1]\n",
        "  referencearea1e=[0, 0.462, 0.208, 0.788]\n",
        "  referencearea1h=[0, 0.178, 0.208, 0.565]\n",
        "  facilitationarea1e=[0, 0.788, 0.208, 0.94]\n",
        "  facilitationarea1h=[0, 0, 0.208, 0.178]\n",
        "  maintaskareae=[0.208, 0.462, 0.542, 0.94]\n",
        "  maintaskareah=[0, 0, 0.542, 0.565]\n",
        "  referencearea2e=[0.542, 0.462, 0.86, 0.94]\n",
        "  referencearea2h=[0.542, 0, 0.86, 0.565]\n",
        "  facilitationarea2e=[0.86, 0.462, 1, 0.94]\n",
        "  facilitationarea2h=[0.86, 0, 1, 0.565]\n",
        "  badatae=[]\n",
        "  badatah=[]\n",
        "  radata1e=[]\n",
        "  radata1h=[]\n",
        "  fadata1e=[]\n",
        "  fadata1h=[]\n",
        "  madata1e=[]\n",
        "  madata1h=[]\n",
        "  radata2e=[]\n",
        "  radata2h=[]\n",
        "  fadata2e=[]\n",
        "  fadata2h=[]\n",
        "  for i in range(eye.shape[0]):\n",
        "    if eye.loc[i].x>=backgroundareae[0] and eye.loc[i].x<=backgroundareae[2] and eye.loc[i].y>=backgroundareae[1] and eye.loc[i].y<=backgroundareae[3]:\n",
        "      badatae.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=referencearea1e[0] and eye.loc[i].x<=referencearea1e[2] and eye.loc[i].y>=referencearea1e[1] and eye.loc[i].y<=referencearea1e[3]:\n",
        "      radata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=facilitationarea1e[0] and eye.loc[i].x<=facilitationarea1e[2] and eye.loc[i].y>=facilitationarea1e[1] and eye.loc[i].y<=facilitationarea1e[3]:\n",
        "      fadata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=maintaskareae[0] and eye.loc[i].x<=maintaskareae[2] and eye.loc[i].y>=maintaskareae[1] and eye.loc[i].y<=maintaskareae[3]:\n",
        "      madata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=referencearea2e[0] and eye.loc[i].x<=referencearea2e[2] and eye.loc[i].y>=referencearea2e[1] and eye.loc[i].y<=referencearea2e[3]:\n",
        "      radata2e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=facilitationarea2e[0] and eye.loc[i].x<=facilitationarea2e[2] and eye.loc[i].y>=facilitationarea2e[1] and eye.loc[i].y<=facilitationarea2e[3]:\n",
        "      fadata2e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "  for i in range(hand.shape[0]):\n",
        "    if hand.loc[i].x>=backgroundareah[0] and hand.loc[i].x<=backgroundareah[2] and hand.loc[i].y>=backgroundareah[1] and hand.loc[i].y<=backgroundareah[3]:\n",
        "      badatah.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=referencearea1h[0] and hand.loc[i].x<=referencearea1h[2] and hand.loc[i].y>=referencearea1h[1] and hand.loc[i].y<=referencearea1h[3]:\n",
        "      radata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=facilitationarea1h[0] and hand.loc[i].x<=facilitationarea1h[2] and hand.loc[i].y>=facilitationarea1h[1] and hand.loc[i].y<=referencearea1h[3]:\n",
        "      fadata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=maintaskareah[0] and hand.loc[i].x<=maintaskareah[2] and hand.loc[i].y>=maintaskareah[1] and hand.loc[i].y<=maintaskareah[3]:\n",
        "      madata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=referencearea2h[0] and hand.loc[i].x<=referencearea2h[2] and hand.loc[i].y>=referencearea2h[1] and hand.loc[i].y<=referencearea2h[3]:\n",
        "      radata2h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=facilitationarea2h[0] and hand.loc[i].x<=facilitationarea2h[2] and hand.loc[i].y>=facilitationarea2h[1] and hand.loc[i].y<=facilitationarea2h[3]:\n",
        "      fadata2h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "\n",
        "  return badatae, badatah, radata1e, radata1h, fadata1e, fadata1h, madata1e, madata1h, radata2e, radata2h, fadata2e, fadata2h"
      ],
      "metadata": {
        "id": "qokC79w6o8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def genfiltereddata(user_type, list):\n",
        "  resulteye=[]\n",
        "  resulthand=[]\n",
        "  maxlen=0\n",
        "  eye_basic_url='/Users/qyuvks/emily/AAAI/data_set/Eye_'\n",
        "  hand_basic_url='/Users/qyuvks/emily/AAAI/data_set/Hand_'\n",
        "  if (user_type=='ASD'):\n",
        "    numOfUser=9\n",
        "    eye_basic_url+=\"ASD_\"\n",
        "    hand_basic_url+='ASD_'\n",
        "  else:\n",
        "    eye_basic_url+=\"TD_\"\n",
        "    hand_basic_url+='TD_'\n",
        "    numOfUser=17\n",
        "  for i in range(1, numOfUser+1):\n",
        "    for j in range(0,2):\n",
        "      c_eye_url=eye_basic_url+'U'+str(i)+\"_Active_\"+str(j)+\".xlsx\"\n",
        "      c_hand_url=hand_basic_url+'U'+str(i)+\"_Active_\"+str(j)+\".xlsx\"\n",
        "      #asd_eye_data=pd.DataFrame()\n",
        "      try:\n",
        "        badatae, badatah, radata1e, radata1h, fadata1e, fadata1h, madata1e, madata1h, radata2e, radata2h, fadata2e, fadata2h = filtereddata(c_eye_url, c_hand_url)\n",
        "        regions = {1: badatae, 2: badatah, 3: radata1e, 4: radata1h, 5: fadata1e, 6: fadata1h, 7: madata1e, 8: madata1h, 9: radata2e, 10: radata2h, 11: fadata2e, 12: fadata2h}\n",
        "        lenh=0\n",
        "        lene=0\n",
        "        for i in range(len(list)):\n",
        "          if len(regions[list[i]])!=0 and len(regions[list[i]+1])!=0:\n",
        "            resulteye.append(regions[list[i]])\n",
        "            resulthand.append(regions[list[i]+1])\n",
        "            lene+=len(regions[list[i]])\n",
        "            lenh+=len(regions[list[i]])\n",
        "        if lene>maxlen or lenh>maxlen:\n",
        "          maxlen=max(lene, lenh)\n",
        "      except IOError:\n",
        "        print(\"\")\n",
        "  return resulteye, resulthand, maxlen\n"
      ],
      "metadata": {
        "id": "qXfudh3FpAyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM Model Used"
      ],
      "metadata": {
        "id": "SnnOe9APpC5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_model4(input_data, output_data, input_max_len, output_max_len, input_features, output_features, latent_dim, number_of_epochs):\n",
        "    # Pad the input and output sequences to their respective maximum lengths\n",
        "    input_data_padded = pad_sequences(input_data, maxlen=input_max_len, padding='post', dtype='float32', value=0)\n",
        "    output_data_padded = pad_sequences(output_data, maxlen=output_max_len, padding='post', dtype='float32', value=0)\n",
        "\n",
        "    # Shift the output sequences to create the target sequences\n",
        "    decoder_target_data = np.roll(output_data_padded, shift=-1, axis=1)\n",
        "    decoder_target_data[:, -1, :] = 0  # Reset last time step to 0 (zero padding)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None, input_features), name='encoder_inputs')\n",
        "    encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_LSTM')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None, output_features), name='decoder_inputs')\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_LSTM')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = TimeDistributed(Dense(output_features, activation='linear'))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn encoder_inputs and decoder_inputs into decoder_outputs\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
        "\n",
        "    # Fit the model\n",
        "    result = model.fit([input_data_padded, output_data_padded], decoder_target_data,\n",
        "                       batch_size=50, epochs=number_of_epochs, validation_split=0.2)\n",
        "\n",
        "    final_loss = result.history['val_loss'][-1]\n",
        "    print(f'Final validation loss: {final_loss}')\n",
        "\n",
        "    # Define the encoder model (used for encoding input sequences to their states)\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    # Define the decoder model (used for generating output sequences given the encoded states)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "  # Extract the number of epochs\n",
        "    epochs = range(1, number_of_epochs + 1)\n",
        "\n",
        "# Extract the validation loss from the history\n",
        "    val_loss = result.history['val_loss']\n",
        "    return model, encoder_model, decoder_model, val_loss\n"
      ],
      "metadata": {
        "id": "EjBAx0SqpCfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Statistical Tests"
      ],
      "metadata": {
        "id": "SXCTvblXpIQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#order is meanASD, meanTD, std_asd, std_td, t_statistic, p_value\n",
        "Statresults = []\n",
        "X=[]\n",
        "Ytd=[]\n",
        "Yasd=[]\n",
        "# Show plot\n",
        "areas = {1: ['background', 1], 2: ['main task', 7], 3: ['not background', 3, 5, 7, 9, 11], 4: ['facilitation', 5, 11], 5: ['reference', 3, 9]}\n",
        "for i in range(1,6):\n",
        "  numareas = areas[i]\n",
        "  asdfit=[]\n",
        "  tdfit=[]\n",
        "  inputtd, outputtd, max_lentd=genfiltereddata('TD', numareas[1:])\n",
        "  inputasd, outputasd, max_lenasd=genfiltereddata('ASD', numareas[1:])\n",
        "  if len(inputasd)>0 and len(inputtd)>0:\n",
        "    for i in range(3):\n",
        "      modeltd, encoder_modeltd, decoder_modeltd, val_losstd = LSTM_model4(inputtd, outputtd, max_lentd, max_lentd, 2, 2, 150, 1000)\n",
        "      modelasd, encoder_modelasd, decoder_modelasd, val_lossasd=LSTM_model4(inputasd, outputasd, max_lenasd, max_lenasd, 2, 2, 150, 1000)\n",
        "      asdfit.append(min(val_lossasd))\n",
        "      tdfit.append(min(val_losstd))\n",
        "  t_statistic, p_value = stats.ttest_ind(asdfit, tdfit)\n",
        "  meanASD = statistics.mean(asdfit)\n",
        "  meanTD = statistics.mean(tdfit)\n",
        "  std_asd = statistics.pstdev(asdfit)\n",
        "  std_td = statistics.pstdev(tdfit)\n",
        "  Statresults.append([numareas[0],meanASD, meanTD, std_asd, std_td, t_statistic, p_value])\n",
        "  X.append(numareas[0])\n",
        "  Ytd.append(meanTD)\n",
        "  Yasd.append(meanASD)\n",
        "# Create DataFrame\n",
        "dataTD = pd.DataFrame({'x': X, 'y': Ytd})\n",
        "sns.scatterplot(data=dataTD, x='x', y='y', color='blue')\n",
        "dataASD = pd.DataFrame({'x': X, 'y': Yasd})\n",
        "sns.scatterplot(data=dataASD, x='x', y='y', color='red')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "plt.xlabel('area')\n",
        "plt.ylabel('validation error')\n",
        "plt.title('area vs error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(X, Ytd, Yasd)\n",
        "print(Statresults)"
      ],
      "metadata": {
        "id": "lhiAiOXPpJds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}