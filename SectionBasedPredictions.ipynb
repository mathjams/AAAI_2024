{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWCEloPb/8dciNS2j52RAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/AAAI_2024/blob/main/SectionBasedPredictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at Fixations that Lie in Specific Sections\n"
      ],
      "metadata": {
        "id": "tvkWWIOo1-6j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfpoYpO41w0g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Embedding, Concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, TimeDistributed, Embedding, Concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import scipy.stats as stats\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Filtering into Specific Sections\n"
      ],
      "metadata": {
        "id": "aROw8izE2kK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtereddata(urleye, urlhand):\n",
        "  eye_data=pd.read_excel(urleye)\n",
        "  hand_data=pd.read_excel(urlhand)\n",
        "  eye=eye_data.copy()\n",
        "  hand=hand_data.copy()\n",
        "  np.array(eye)\n",
        "  np.array(hand)\n",
        "  #each area is described as a rectangle with [Xmin, Ymin, Xmax, Ymax]\n",
        "  backgroundareae=[0, 0.09, 1, 0.462]\n",
        "  backgroundareah=[0, 0.565, 1, 1]\n",
        "  referencearea1e=[0, 0.462, 0.208, 0.788]\n",
        "  referencearea1h=[0, 0.178, 0.208, 0.565]\n",
        "  facilitationarea1e=[0, 0.788, 0.208, 0.94]\n",
        "  facilitationarea1h=[0, 0, 0.208, 0.178]\n",
        "  maintaskareae=[0.208, 0.462, 0.542, 0.94]\n",
        "  maintaskareah=[0, 0, 0.542, 0.565]\n",
        "  referencearea2e=[0.542, 0.462, 0.86, 0.94]\n",
        "  referencearea2h=[0.542, 0, 0.86, 0.565]\n",
        "  facilitationarea2e=[0.86, 0.462, 1, 0.94]\n",
        "  facilitationarea2h=[0.86, 0, 1, 0.565]\n",
        "  badatae=[]\n",
        "  badatah=[]\n",
        "  radata1e=[]\n",
        "  radata1h=[]\n",
        "  fadata1e=[]\n",
        "  fadata1h=[]\n",
        "  madata1e=[]\n",
        "  madata1h=[]\n",
        "  radata2e=[]\n",
        "  radata2h=[]\n",
        "  fadata2e=[]\n",
        "  fadata2h=[]\n",
        "  for i in range(eye.shape[0]):\n",
        "    if eye.loc[i].x>=backgroundareae[0] and eye.loc[i].x<=backgroundareae[2] and eye.loc[i].y>=backgroundareae[1] and eye.loc[i].y<=backgroundareae[3]:\n",
        "      badatae.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=referencearea1e[0] and eye.loc[i].x<=referencearea1e[2] and eye.loc[i].y>=referencearea1e[1] and eye.loc[i].y<=referencearea1e[3]:\n",
        "      radata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=facilitationarea1e[0] and eye.loc[i].x<=facilitationarea1e[2] and eye.loc[i].y>=facilitationarea1e[1] and eye.loc[i].y<=facilitationarea1e[3]:\n",
        "      fadata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=maintaskareae[0] and eye.loc[i].x<=maintaskareae[2] and eye.loc[i].y>=maintaskareae[1] and eye.loc[i].y<=maintaskareae[3]:\n",
        "      madata1e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=referencearea2e[0] and eye.loc[i].x<=referencearea2e[2] and eye.loc[i].y>=referencearea2e[1] and eye.loc[i].y<=referencearea2e[3]:\n",
        "      radata2e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "    if eye.loc[i].x>=facilitationarea2e[0] and eye.loc[i].x<=facilitationarea2e[2] and eye.loc[i].y>=facilitationarea2e[1] and eye.loc[i].y<=facilitationarea2e[3]:\n",
        "      fadata2e.append([eye.loc[i].x, eye.loc[i].y])\n",
        "  for i in range(hand.shape[0]):\n",
        "    if hand.loc[i].x>=backgroundareah[0] and hand.loc[i].x<=backgroundareah[2] and hand.loc[i].y>=backgroundareah[1] and hand.loc[i].y<=backgroundareah[3]:\n",
        "      badatah.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=referencearea1h[0] and hand.loc[i].x<=referencearea1h[2] and hand.loc[i].y>=referencearea1h[1] and hand.loc[i].y<=referencearea1h[3]:\n",
        "      radata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=facilitationarea1h[0] and hand.loc[i].x<=facilitationarea1h[2] and hand.loc[i].y>=facilitationarea1h[1] and hand.loc[i].y<=referencearea1h[3]:\n",
        "      fadata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=maintaskareah[0] and hand.loc[i].x<=maintaskareah[2] and hand.loc[i].y>=maintaskareah[1] and hand.loc[i].y<=maintaskareah[3]:\n",
        "      madata1h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=referencearea2h[0] and hand.loc[i].x<=referencearea2h[2] and hand.loc[i].y>=referencearea2h[1] and hand.loc[i].y<=referencearea2h[3]:\n",
        "      radata2h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "    if hand.loc[i].x>=facilitationarea2h[0] and hand.loc[i].x<=facilitationarea2h[2] and hand.loc[i].y>=facilitationarea2h[1] and hand.loc[i].y<=facilitationarea2h[3]:\n",
        "      fadata2h.append([hand.loc[i].x, hand.loc[i].y])\n",
        "\n",
        "  return badatae, badatah, radata1e, radata1h, fadata1e, fadata1h, madata1e, madata1h, radata2e, radata2h, fadata2e, fadata2h"
      ],
      "metadata": {
        "id": "HWGmzb3k8rtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def genfiltereddata(user_type, list):\n",
        "  resulteye=[]\n",
        "  resulthand=[]\n",
        "  maxlen=0\n",
        "  eye_basic_url='/Users/qyuvks/emily/AAAI/data_set/Eye_'\n",
        "  hand_basic_url='/Users/qyuvks/emily/AAAI/data_set/Hand_'\n",
        "  if (user_type=='ASD'):\n",
        "    numOfUser=9\n",
        "    eye_basic_url+=\"ASD_\"\n",
        "    hand_basic_url+='ASD_'\n",
        "  else:\n",
        "    eye_basic_url+=\"TD_\"\n",
        "    hand_basic_url+='TD_'\n",
        "    numOfUser=17\n",
        "  for i in range(1, numOfUser+1):\n",
        "    for j in range(0,2):\n",
        "      c_eye_url=eye_basic_url+'U'+str(i)+\"_Active_\"+str(j)+\".xlsx\"\n",
        "      c_hand_url=hand_basic_url+'U'+str(i)+\"_Active_\"+str(j)+\".xlsx\"\n",
        "      #asd_eye_data=pd.DataFrame()\n",
        "      try:\n",
        "        badatae, badatah, radata1e, radata1h, fadata1e, fadata1h, madata1e, madata1h, radata2e, radata2h, fadata2e, fadata2h = filtereddata(c_eye_url, c_hand_url)\n",
        "        regions = {1: badatae, 2: badatah, 3: radata1e, 4: radata1h, 5: fadata1e, 6: fadata1h, 7: madata1e, 8: madata1h, 9: radata2e, 10: radata2h, 11: fadata2e, 12: fadata2h}\n",
        "        lenh=0\n",
        "        lene=0\n",
        "        for i in range(len(list)):\n",
        "          if len(regions[list[i]])!=0 and len(regions[list[i]+1])!=0:\n",
        "            resulteye.append(regions[list[i]])\n",
        "            resulthand.append(regions[list[i]+1])\n",
        "            lene+=len(regions[list[i]])\n",
        "            lenh+=len(regions[list[i]])\n",
        "        if lene>maxlen or lenh>maxlen:\n",
        "          maxlen=max(lene, lenh)\n",
        "      except IOError:\n",
        "        print(\"\")\n",
        "  return resulteye, resulthand, maxlen"
      ],
      "metadata": {
        "id": "E1wtNxfF11fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Loss Function to Ensure Accurate Comparison"
      ],
      "metadata": {
        "id": "76SP5H9k2uKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_mse_loss(y_true, y_pred):\n",
        "    # Create a mask to ignore the padded values (assumes padding is done with 0s)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
        "    # Compute MSE and apply the mask\n",
        "    mse = tf.square(y_true - y_pred) * mask\n",
        "    # Take the mean of the MSE, dividing by the non-zero entries in the mask\n",
        "    return tf.reduce_sum(mse) / tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "VXtV86RY2tra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU Model Used"
      ],
      "metadata": {
        "id": "fgsbB63Y8bL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_model4(input_data, output_data, input_max_len, output_max_len, input_features, output_features, latent_dim, number_of_epochs):\n",
        "    # Pad the input and output sequences to their respective maximum lengths\n",
        "    input_data_padded = pad_sequences(input_data, maxlen=input_max_len, padding='post', dtype='float32', value=0)\n",
        "    output_data_padded = pad_sequences(output_data, maxlen=output_max_len, padding='post', dtype='float32', value=0)\n",
        "\n",
        "    # Shift the output sequences to create the target sequences\n",
        "    decoder_target_data = np.roll(output_data_padded, shift=-1, axis=1)\n",
        "    decoder_target_data[:, -1, :] = 0  # Reset last time step to 0 (zero padding)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None, input_features), name='encoder_inputs')\n",
        "    encoder_gru = GRU(latent_dim, return_state=True, name='encoder_GRU')\n",
        "    encoder_outputs, state_h = encoder_gru(encoder_inputs)\n",
        "    encoder_states = [state_h]  # GRU only has a single hidden state\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None, output_features), name='decoder_inputs')\n",
        "    decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder_GRU')\n",
        "    decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = TimeDistributed(Dense(output_features, activation='linear'))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn encoder_inputs and decoder_inputs into decoder_outputs\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=masked_mse_loss)\n",
        "\n",
        "    # Fit the model\n",
        "    result = model.fit(\n",
        "        [input_data_padded, output_data_padded],\n",
        "        decoder_target_data,\n",
        "        batch_size=50,\n",
        "        epochs=number_of_epochs,\n",
        "        validation_split=0.2,\n",
        "        verbose=1  # Set verbose=1 to print loss at each epoch\n",
        "    )\n",
        "\n",
        "    # Extract the validation loss from the history\n",
        "    val_loss = result.history['val_loss']\n",
        "    loss = result.history['loss']\n",
        "\n",
        "    # Print the minimum validation loss\n",
        "    min_val_loss = min(val_loss)\n",
        "    print(f'Minimum validation loss: {min_val_loss}')\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    plot_loss(result.history)\n",
        "\n",
        "    # Define the encoder model (used for encoding input sequences to their states)\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    # Define the decoder model (used for generating output sequences given the encoded states)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h]\n",
        "\n",
        "    decoder_outputs, state_h_dec = decoder_gru(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h_dec]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    # Return model and training history\n",
        "    return min_val_loss"
      ],
      "metadata": {
        "id": "zy2VTsa_8fBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Tests"
      ],
      "metadata": {
        "id": "C3bdPQ-88m68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#order is meanASD, meanTD, std_asd, std_td, t_statistic, p_value\n",
        "Statresults = []\n",
        "X=[]\n",
        "Ytd=[]\n",
        "Yasd=[]\n",
        "# Show plot\n",
        "areas = {1: ['main task', 7], 2: ['facilitation', 5, 11], 3: ['reference', 3, 9]}\n",
        "for i in range(1,4):\n",
        "  numareas = areas[i]\n",
        "  asdfit=[]\n",
        "  tdfit=[]\n",
        "  inputtd, outputtd, max_lentd=genfiltereddata('TD', numareas[1:])\n",
        "  inputasd, outputasd, max_lenasd=genfiltereddata('ASD', numareas[1:])\n",
        "  if len(inputasd)>4 and len(inputtd)>4:\n",
        "    for i in range(3):\n",
        "      val_losstd = GRU_model4(inputtd, outputtd, max_lentd, max_lentd, 2, 2, 64, 1000)\n",
        "      val_lossasd=GRU_model4(inputasd, outputasd, max_lenasd, max_lenasd, 2, 2, 64, 1000)\n",
        "      asdfit.append(val_lossasd)\n",
        "      tdfit.append(val_losstd)\n",
        "    t_statistic, p_value = stats.ttest_ind(asdfit, tdfit)\n",
        "    meanASD = statistics.mean(asdfit)\n",
        "    meanTD = statistics.mean(tdfit)\n",
        "    std_asd = statistics.pstdev(asdfit)\n",
        "    std_td = statistics.pstdev(tdfit)\n",
        "    Statresults.append([numareas[0],meanASD, meanTD, std_asd, std_td, t_statistic, p_value])\n",
        "    X.append(numareas[0])\n",
        "    Ytd.append(meanTD)\n",
        "    Yasd.append(meanASD)\n",
        "# Create DataFrame\n",
        "dataTD = pd.DataFrame({'x': X, 'y': Ytd})\n",
        "sns.scatterplot(data=dataTD, x='x', y='y', color='blue')\n",
        "dataASD = pd.DataFrame({'x': X, 'y': Yasd})\n",
        "sns.scatterplot(data=dataASD, x='x', y='y', color='red')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "plt.xlabel('area')\n",
        "plt.ylabel('validation error')\n",
        "plt.title('area vs error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(X, Ytd, Yasd)\n",
        "print(Statresults)"
      ],
      "metadata": {
        "id": "IpYXJ1d58pG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}